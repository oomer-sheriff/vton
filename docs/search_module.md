# Module: Search & Discovery Engine

**Goal**: Allow users to intuitive find garments using natural language ("summer floral dress") or specific filters ("Color: Red"), leveraging the AI-extracted metadata.

## 1. The Core Problem
We have a catalogue of images. Traditional SQL search (`SELECT * FROM clothes WHERE name LIKE '%dress%'`) is brittle.
Since we already use a **VLM** (Vision Language Model) to tag our clothes during Ingestion, we have rich data. We can use this for two types of search:
1.  **Faceted Search**: Precise filtering (e.g. "Color = Red" AND "Size = M").
2.  **Semantic Search**: Conceptual searching (e.g. "Something for a beach wedding").

## 2. Architecture: Hybrid Search (Postgres + pgvector)

We utilizing a single database technology to keep the stack simple: **PostgreSQL**.

### A. Faceted Search (JSONB)
The metadata from the VLM is stored in a `JSONB` column. Postgres allows high-performance indexing on JSON fields.
*   **Query**:
    ```sql
    SELECT * FROM garments 
    WHERE metadata->>'category' = 'dress' 
    AND metadata->>'color' = 'red';
    ```
*   **Index**: GIN Index on the metadata column ensures this is instant even with millions of items.

### B. Semantic Search (pgvector)
We will generate **Text Embeddings** for the garment descriptions (generated by the VLM).
1.  **Ingestion**: When VLM generates a description (e.g., "A breezy floral red dress suitable for summer"), we pass this text to an embedding model (e.g., `text-embedding-3-small` or local `all-MiniLM-L6-v2`).
2.  **Storage**: Save the resulting vector (array of floats) in a `vector` column in Postgres.
3.  **Query**:
    *   User types: "outfit for a gala".
    *   System converts query to vector.
    *   SQL: `SELECT * FROM garments ORDER BY embedding <=> query_vector LIMIT 20;`

## 3. Implementation Details

### Database Schema Updates
```sql
CREATE EXTENSION vector;

ALTER TABLE garments ADD COLUMN description_embedding vector(1536); -- If using OpenAI
-- OR
ALTER TABLE garments ADD COLUMN description_embedding vector(384); -- If using local MiniLM

CREATE INDEX ON garments USING hnsw (description_embedding vector_cosine_ops);
```

### API Endpoints

`GET /api/v1/search`
*   **Params**:
    *   `q`: (Optional) Free text search query.
    *   `filters`: (Optional) JSON string for faceted filters `{"color": "blue"}`.
*   **Logic**:
    *   If `q` is present: Run Vector Search.
    *   If `filters` are present: Apply SQL `WHERE` clauses on JSONB.
    *   If both: Combine (Hybrid Search).

## 4. UI / UX
*   **Search Bar**: "Ask for anything..." (Semantic).
*   **Filter Chips**: "Dress", "Casual", "Red" (Generated dynamically from available metadata tags).
*   **Result Ordering**: Sort by "Relevance" (Vector Distance).

## Summary
By using the **Metadata** we established in the Ingestion Pipeline, we turn a static image grid into an intelligent discovery engine without needing complex external search services like Elasticsearch or Pinecone.
